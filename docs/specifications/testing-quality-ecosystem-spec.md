# Testing & Quality Analysis Ecosystem Specification

> **Version**: 1.0.0
> **Date**: 2026-01-12
> **Status**: Active
> **Spec ID**: BATUTA-TESTING-001

## Executive Summary

The PAIML ecosystem provides three complementary tools for testing and quality analysis, each with distinct but non-overlapping responsibilities:

| Tool | Domain | Primary Function |
|------|--------|------------------|
| **pmat** | Static Analysis | Code quality, TDG, SATD, complexity |
| **oip** | Defect Intelligence | ML defect classification, fault localization |
| **probar** | Runtime Testing | WASM testing, browser automation, visual regression |

**Critical Insight**: These tools are NOT substitutes for each other. A project needs all three for comprehensive quality assurance.

---

## Tool Capability Matrix

### Feature Comparison

| Capability | pmat | oip | probar |
|------------|------|-----|--------|
| **Static Analysis** |
| SATD Detection | âœ… 4-severity, 355+ patterns | âŒ | âŒ |
| Cyclomatic Complexity | âœ… CC metrics | âŒ | âŒ |
| Cognitive Complexity | âœ… | âŒ | âŒ |
| Dead Code Detection | âœ… | âŒ | âŒ |
| Code Duplication | âœ… | âŒ | âŒ |
| TDG Scoring | âœ… A-F grades | âŒ | âŒ |
| **Defect Intelligence** |
| SBFL Fault Localization | âŒ | âœ… Tarantula, Ochiai, DStar | âœ… Basic Tarantula |
| Commit Classification | âŒ | âœ… ML classifier | âŒ |
| Defect Pattern ML | âŒ | âœ… RandomForest, RAG | âŒ |
| Calibrated Predictions | âŒ | âœ… Phase 7 | âŒ |
| Ensemble Models | âŒ | âœ… Phase 6 | âŒ |
| **Runtime Testing** |
| Browser Automation | âŒ | âŒ | âœ… CDP protocol |
| Visual Regression | âŒ | âŒ | âœ… Perceptual diff |
| WASM Coverage | âŒ | âŒ | âœ… Block-level |
| TUI Testing | âŒ | âŒ | âœ… Presentar support |
| Pixel Coverage | âŒ | âŒ | âœ… Heatmaps |
| **Algorithmic Analysis** |
| O(n) Complexity Detection | âŒ | âŒ | âœ… Curve fitting |
| Rc/RefCell Linting | âŒ | âŒ | âœ… AST-based |
| WASM Threading Compliance | âŒ | âŒ | âœ… |

---

## Tool 1: pmat (Static Analysis)

### Purpose

Zero-configuration code quality analysis for 17+ languages. Provides actionable metrics without running code.

### Key Capabilities

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PMAT CAPABILITIES                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  TDG Scoring          â”‚ A-F grades, 6 weighted metrics  â”‚
â”‚  SATD Detection       â”‚ TODO/FIXME/HACK with severity   â”‚
â”‚  Complexity Analysis  â”‚ Cyclomatic + Cognitive          â”‚
â”‚  Dead Code            â”‚ Unused functions/modules        â”‚
â”‚  Code Duplication     â”‚ Clone detection                 â”‚
â”‚  Security Scan        â”‚ Basic vulnerability patterns    â”‚
â”‚  Documentation        â”‚ Coverage and quality            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Example Output

```bash
$ pmat quality-gate

Quality Gate: FAILED
Total violations: 475

  Complexity:      64 violations
  Dead code:       6 violations
  Technical debt:  355 violations (17 critical)
  Code entropy:    41 violations
  Duplicates:      6 violations
```

### When to Use

- Pre-commit quality gates
- CI/CD pipeline checks
- Technical debt assessment
- Code review preparation
- Refactoring prioritization

---

## Tool 2: oip (Organizational Intelligence Plugin)

### Purpose

ML-powered defect pattern analysis and fault localization using git history and coverage data.

### Key Capabilities

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    OIP CAPABILITIES                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Tarantula SBFL       â”‚ Spectrum-based fault localizationâ”‚
â”‚  Ochiai/DStar         â”‚ Alternative SBFL formulas        â”‚
â”‚  Commit Classificationâ”‚ ML labeling of defect types      â”‚
â”‚  Training Extraction  â”‚ Git history â†’ training data      â”‚
â”‚  RAG Enhancement      â”‚ trueno-rag knowledge retrieval   â”‚
â”‚  Ensemble Models      â”‚ Weighted multi-model predictions â”‚
â”‚  Calibrated Output    â”‚ Confidence-calibrated scores     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Example Output

```bash
$ oip extract-training-data --repo ../whisper.apr

Training Data Statistics:
  Total examples: 13
  Avg confidence: 0.82

Class Distribution:
  TraitBounds: 3 (23.1%)
  ASTTransform: 3 (23.1%)
  ConfigurationErrors: 3 (23.1%)
  OwnershipBorrow: 2 (15.4%)
  ConcurrencyBugs: 1 (7.7%)
  SecurityVulnerabilities: 1 (7.7%)
```

### Fault Localization

```bash
$ oip localize \
    --passed-coverage passed.lcov \
    --failed-coverage failed.lcov \
    --formula tarantula \
    --top-n 10

ğŸ¯ Tarantula Hotspot Report
   Line  | Suspiciousness | Status
   ------|----------------|--------
   142   | 0.950          | ğŸ”´ HIGH
   287   | 0.823          | ğŸ”´ HIGH
   56    | 0.612          | ğŸŸ¡ MEDIUM
```

### When to Use

- Post-test-failure debugging
- Defect pattern analysis across organization
- Training ML models on historical defects
- Root cause analysis
- Bug triage prioritization

---

## Tool 3: probar (Runtime Testing)

### Purpose

Rust-native testing framework for WASM games and web applications. Browser automation, visual regression, and pixel-level coverage.

### Key Capabilities

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   PROBAR CAPABILITIES                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  CDP Browser Control  â”‚ Chrome DevTools Protocol        â”‚
â”‚  Visual Regression    â”‚ Perceptual diff, mask regions   â”‚
â”‚  WASM Coverage        â”‚ Block/superblock instrumentationâ”‚
â”‚  Pixel Coverage       â”‚ Heatmap visualization           â”‚
â”‚  TUI Testing          â”‚ Presentar YAML falsification    â”‚
â”‚  Tarantula SBFL       â”‚ Basic fault localization        â”‚
â”‚  O(n) Detection       â”‚ Empirical complexity curves     â”‚
â”‚  Rc/RefCell Linting   â”‚ AST-based state sync detection  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Example: Visual Regression

```rust
use jugar_probar::{VisualRegressionTester, VisualRegressionConfig};

let tester = VisualRegressionTester::new(
    VisualRegressionConfig::default()
        .with_threshold(0.02)
        .with_color_threshold(10)
);

let result = tester.compare_images(&baseline, &current)?;
assert!(result.matches, "Visual regression: {}% diff", result.diff_percentage);
```

### Example: Presentar TUI Testing

```rust
use jugar_probar::{TerminalSnapshot, TerminalAssertion};

let snapshot = TerminalSnapshot::from_string(output, 80, 24);

let assertions = [
    TerminalAssertion::Contains("CPU".into()),
    TerminalAssertion::NotContains("ERROR".into()),
    TerminalAssertion::CharAt { x: 0, y: 0, expected: 'â”Œ' },
];

for assertion in &assertions {
    assertion.check(&snapshot)?;
}
```

### When to Use

- WASM game/application testing
- Browser-based UI testing
- Visual regression in CI/CD
- TUI application validation
- Pixel-level coverage analysis

---

## Integration Workflow

### Recommended Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    QUALITY ASSURANCE PIPELINE                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                  â”‚
â”‚  Phase 1: Static Analysis (pmat)                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ pmat quality-gate                                        â”‚    â”‚
â”‚  â”‚ â†’ TDG score, SATD, complexity, dead code, duplicates    â”‚    â”‚
â”‚  â”‚ â†’ FAIL if violations > threshold                         â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                           â†“                                      â”‚
â”‚  Phase 2: Runtime Testing (probar)                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ cargo test + probar coverage                             â”‚    â”‚
â”‚  â”‚ â†’ Unit tests, integration tests                          â”‚    â”‚
â”‚  â”‚ â†’ Visual regression (if UI)                              â”‚    â”‚
â”‚  â”‚ â†’ Generate LCOV coverage                                 â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                           â†“                                      â”‚
â”‚  Phase 3: Fault Analysis (oip) [on test failure]                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ oip localize --passed-coverage --failed-coverage        â”‚    â”‚
â”‚  â”‚ â†’ Tarantula SBFL hotspot report                          â”‚    â”‚
â”‚  â”‚ â†’ Defect pattern classification                          â”‚    â”‚
â”‚  â”‚ â†’ Prioritized debugging targets                          â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Makefile Integration

```makefile
# Combined quality pipeline
.PHONY: quality

quality: static-analysis test fault-analysis

static-analysis:
	@echo "Phase 1: Static Analysis (pmat)"
	pmat quality-gate --strict

test:
	@echo "Phase 2: Runtime Testing (probar)"
	cargo test --all-features
	cargo llvm-cov --lcov --output-path lcov.info

fault-analysis:
	@echo "Phase 3: Fault Analysis (oip) - only on failure"
	@if [ -f failed-tests.lcov ]; then \
		oip localize \
			--passed-coverage lcov.info \
			--failed-coverage failed-tests.lcov \
			--top-n 20; \
	fi
```

---

## Gap Analysis: What Each Tool Cannot Do

### pmat Cannot:

- Run tests or execute code
- Perform fault localization (needs runtime data)
- Train ML models on defects
- Test visual UI/pixels
- Analyze WASM binaries

### oip Cannot:

- Detect SATD (TODO/FIXME)
- Calculate cyclomatic complexity
- Find dead code or duplicates
- Run browser automation
- Generate TDG scores

### probar Cannot:

- Detect SATD patterns
- Analyze code without executing it
- Train defect classification models
- Calculate TDG scores
- Find code duplication

---

## Real-World Analysis Results

### whisper.apr (analyzed 2026-01-12)

| Tool | Finding |
|------|---------|
| **pmat** | 475 violations: 355 SATD (17 critical), 64 complexity, 6 dead code |
| **oip** | 13 defect patterns: TraitBounds 23%, ASTTransform 23%, ConfigErrors 23% |
| **probar** | 435 `unwrap()` calls detected via Rc/RefCell linting |

### interactive.paiml.com (analyzed 2026-01-12)

| Tool | Finding |
|------|---------|
| **pmat** | 561 violations: 439 complexity, 88 duplicates, 21 SATD |
| **oip** | 146 defect patterns: ASTTransform 36%, OwnershipBorrow 30% |
| **probar** | Visual regression capability for 15 WASM demos |

---

## Version Information

| Tool | Current Version | Repository |
|------|-----------------|------------|
| pmat | 2.213.4 | github.com/paiml/paiml-mcp-agent-toolkit |
| oip | 0.3.1 | github.com/paiml/organizational-intelligence-plugin |
| probar | 0.2.x | github.com/paiml/probar (crates.io: jugar-probar) |

---

## Conclusion

**No single tool provides complete quality assurance.** The PAIML ecosystem requires:

1. **pmat** for static analysis before code runs
2. **probar** for runtime testing and visual validation
3. **oip** for post-failure fault localization and defect intelligence

Using all three tools together provides defense-in-depth quality assurance following Toyota Way principles:

- **Jidoka**: pmat catches defects at compile-time
- **Poka-Yoke**: probar's type-safe selectors prevent errors
- **Genchi Genbutsu**: oip's SBFL goes to the source of bugs

---

**Navigate:** [Specifications Index](./README.md) | [Stack Spec](./batuta-stack-spec.md)
