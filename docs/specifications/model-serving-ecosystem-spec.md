# Model Serving Ecosystem Specification v1.0.0

## Overview

Unified interface for local and remote model serving across the ML ecosystem, with native PAIML integration via `realizar`.

```
[REVIEW-001] @alfredo 2024-12-05
Toyota Principle: Genchi Genbutsu (Go and See)
Direct integration with serving backends eliminates abstraction overhead.
Batuta queries actual inference endpoints, not cached proxies.
Status: APPROVED
```

## Ecosystem Landscape

### Local Serving

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     LOCAL MODEL SERVING                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Tool         â”‚ Language â”‚ Format      â”‚ Key Feature             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ realizar     â”‚ Rust     â”‚ GGUF/ST     â”‚ PAIML native, MoE       â”‚
â”‚ Ollama       â”‚ Go       â”‚ GGUF        â”‚ Docker-like UX          â”‚
â”‚ llamafile    â”‚ C        â”‚ GGUF        â”‚ Single executable       â”‚
â”‚ llama.cpp    â”‚ C++      â”‚ GGUF        â”‚ Reference impl          â”‚
â”‚ candle       â”‚ Rust     â”‚ SafeTensors â”‚ HF Rust framework       â”‚
â”‚ vLLM         â”‚ Python   â”‚ PT/ST       â”‚ PagedAttention          â”‚
â”‚ TGI          â”‚ Rust/Py  â”‚ SafeTensors â”‚ HF official server      â”‚
â”‚ LocalAI      â”‚ Go       â”‚ GGUF/*      â”‚ OpenAI-compatible       â”‚
â”‚ LM Studio    â”‚ Electron â”‚ GGUF        â”‚ Desktop GUI             â”‚
â”‚ GPT4All      â”‚ C++      â”‚ GGUF        â”‚ Desktop app             â”‚
â”‚ MLC LLM      â”‚ C++      â”‚ MLC         â”‚ Universal deployment    â”‚
â”‚ ExLlamaV2    â”‚ Python   â”‚ EXL2        â”‚ Extreme quantization    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Legend: PT=PyTorch, ST=SafeTensors, MoE=Mixture of Experts
```

```
[REVIEW-002] @noah 2024-12-05
Toyota Principle: Heijunka (Level Loading)
Multiple backend support balances load across available resources.
Local serving for low-latency; remote for scale-out.
Status: APPROVED
```

### Remote Serving

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    REMOTE MODEL SERVING                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Service          â”‚ Provider    â”‚ Characteristics                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ HF Inference API â”‚ HuggingFace â”‚ Serverless, pay-per-token      â”‚
â”‚ HF Endpoints     â”‚ HuggingFace â”‚ Dedicated, auto-scaling        â”‚
â”‚ Together.ai      â”‚ Together    â”‚ Fast inference, open models    â”‚
â”‚ Replicate        â”‚ Replicate   â”‚ Pay-per-call, easy deploy      â”‚
â”‚ Anyscale         â”‚ Anyscale    â”‚ Ray-based, distributed         â”‚
â”‚ Modal            â”‚ Modal       â”‚ Serverless GPU, Python-native  â”‚
â”‚ Fireworks.ai     â”‚ Fireworks   â”‚ Fast, function calling         â”‚
â”‚ Groq             â”‚ Groq        â”‚ LPU hardware, ultra-fast       â”‚
â”‚ AWS Bedrock      â”‚ Amazon      â”‚ Managed, enterprise            â”‚
â”‚ Azure OpenAI     â”‚ Microsoft   â”‚ OpenAI models, enterprise      â”‚
â”‚ Google Vertex    â”‚ Google      â”‚ PaLM/Gemini, enterprise        â”‚
â”‚ Anthropic API    â”‚ Anthropic   â”‚ Claude models                  â”‚
â”‚ OpenAI API       â”‚ OpenAI      â”‚ GPT models                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

```
[REVIEW-003] @maria 2024-12-05
Toyota Principle: Just-in-Time
Remote APIs enable on-demand inference without idle GPU costs.
Pay only for actual compute consumed.
Status: APPROVED
```

## CLI Interface

### Serve Commands

```bash
# Start local server with realizar
batuta serve start --backend realizar --model ./model.gguf --port 8080

# Start with Ollama backend
batuta serve start --backend ollama --model llama2:7b

# Start with llama.cpp
batuta serve start --backend llamacpp --model ./model.gguf --ctx 4096

# List running servers
batuta serve list

# Stop server
batuta serve stop --port 8080
```

### Query Commands

```bash
# Query local server
batuta serve query "What is the capital of France?" --endpoint localhost:8080

# Query remote API
batuta serve query "Explain transformers" --endpoint together --model mistral-7b

# Benchmark endpoint
batuta serve bench --endpoint localhost:8080 --prompts ./prompts.jsonl
```

### Backend Management

```bash
# List available backends
batuta serve backends

# Check backend health
batuta serve health --backend ollama

# Pull model for backend
batuta serve pull --backend ollama --model codellama:13b
```

```
[REVIEW-004] @carlos 2024-12-05
Toyota Principle: Standardized Work
Unified CLI across all backends reduces cognitive load.
Same commands work for realizar, Ollama, or cloud APIs.
Status: APPROVED
```

## Data Model

```rust
/// Supported serving backends
#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum ServingBackend {
    // Local backends
    Realizar,
    Ollama,
    LlamaCpp,
    Llamafile,
    Candle,
    Vllm,
    Tgi,
    LocalAI,

    // Remote backends
    HuggingFace,
    Together,
    Replicate,
    Anyscale,
    Modal,
    Fireworks,
    Groq,
    OpenAI,
    Anthropic,
    AzureOpenAI,
    AwsBedrock,
    GoogleVertex,
}

/// Backend capabilities
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct BackendCapabilities {
    pub streaming: bool,
    pub function_calling: bool,
    pub vision: bool,
    pub embeddings: bool,
    pub batch_inference: bool,
    pub quantization: Vec<String>,
    pub max_context: usize,
    pub formats: Vec<ModelFormat>,
}

/// Model format
#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum ModelFormat {
    GGUF,
    SafeTensors,
    PyTorch,
    ONNX,
    TensorRT,
    OpenVINO,
    MLC,
    EXL2,
}

/// Inference request
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct InferenceRequest {
    pub prompt: String,
    pub max_tokens: Option<usize>,
    pub temperature: Option<f32>,
    pub top_p: Option<f32>,
    pub stop: Option<Vec<String>>,
    pub stream: bool,
}

/// Inference response
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct InferenceResponse {
    pub text: String,
    pub tokens_generated: usize,
    pub latency_ms: u64,
    pub tokens_per_second: f32,
}
```

```
[REVIEW-005] @elena 2024-12-05
Toyota Principle: Jidoka (Built-in Quality)
Strong typing prevents runtime errors at API boundaries.
Invalid requests fail at compile time, not in production.
Status: APPROVED
```

## Backend Integration Tree

```bash
batuta serve tree
```

```
Model Serving Ecosystem
â”œâ”€â”€ local
â”‚   â”œâ”€â”€ paiml_native
â”‚   â”‚   â””â”€â”€ realizar      âš¡ Rust, GGUF/ST, MoE routing
â”‚   â”œâ”€â”€ gguf_based
â”‚   â”‚   â”œâ”€â”€ ollama        âœ“ Go, Docker-like CLI
â”‚   â”‚   â”œâ”€â”€ llamafile     âœ“ C, single executable
â”‚   â”‚   â”œâ”€â”€ llama.cpp     âœ“ C++, reference impl
â”‚   â”‚   â”œâ”€â”€ localai       âœ“ Go, OpenAI-compatible
â”‚   â”‚   â””â”€â”€ gpt4all       âœ“ C++, desktop app
â”‚   â”œâ”€â”€ rust_native
â”‚   â”‚   â””â”€â”€ candle        âœ“ Rust, HF framework
â”‚   â””â”€â”€ python_based
â”‚       â”œâ”€â”€ vllm          âœ“ PagedAttention
â”‚       â”œâ”€â”€ tgi           âœ“ HF official
â”‚       â””â”€â”€ exllamav2     âœ“ EXL2 quantization
â”œâ”€â”€ remote
â”‚   â”œâ”€â”€ huggingface
â”‚   â”‚   â”œâ”€â”€ inference-api â˜ Serverless
â”‚   â”‚   â””â”€â”€ endpoints     â˜ Dedicated
â”‚   â”œâ”€â”€ inference_providers
â”‚   â”‚   â”œâ”€â”€ together      â˜ Fast, open models
â”‚   â”‚   â”œâ”€â”€ replicate     â˜ Pay-per-call
â”‚   â”‚   â”œâ”€â”€ fireworks     â˜ Function calling
â”‚   â”‚   â””â”€â”€ groq          â˜ LPU, ultra-fast
â”‚   â”œâ”€â”€ serverless_gpu
â”‚   â”‚   â”œâ”€â”€ modal         â˜ Python-native
â”‚   â”‚   â””â”€â”€ anyscale      â˜ Ray-based
â”‚   â””â”€â”€ enterprise
â”‚       â”œâ”€â”€ aws_bedrock   â˜ Amazon managed
â”‚       â”œâ”€â”€ azure_openai  â˜ Microsoft managed
â”‚       â””â”€â”€ google_vertex â˜ Google managed
â””â”€â”€ frontier_apis
    â”œâ”€â”€ openai            â˜ GPT-4, GPT-4o
    â”œâ”€â”€ anthropic         â˜ Claude 3.5
    â””â”€â”€ google            â˜ Gemini
```

```
[REVIEW-006] @david 2024-12-05
Toyota Principle: Visual Management
Tree view provides instant ecosystem comprehension.
Engineers see all options without documentation diving.
Status: APPROVED
```

## PAIML Integration Map

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PAIML Component â”‚ Ecosystem Equiv    â”‚ Integration Type       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ INFERENCE       â”‚                    â”‚                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ realizar        â”‚ llama.cpp          â”‚ âš¡ ALTERNATIVE (Rust)   â”‚
â”‚ realizar        â”‚ Ollama             â”‚ âš¡ ALTERNATIVE (native) â”‚
â”‚ realizar        â”‚ vLLM               â”‚ âš¡ ALTERNATIVE (MoE)    â”‚
â”‚ realizar        â”‚ TGI                â”‚ âš¡ ALTERNATIVE (Rust)   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ FORMAT SUPPORT  â”‚                    â”‚                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ realizar/gguf   â”‚ GGUF ecosystem     â”‚ âœ“ COMPATIBLE (parse)   â”‚
â”‚ realizar/st     â”‚ SafeTensors        â”‚ âœ“ COMPATIBLE (r/w)     â”‚
â”‚ realizar/tok    â”‚ HF tokenizers      â”‚ âœ“ COMPATIBLE (load)    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ ORCHESTRATION   â”‚                    â”‚                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ batuta serve    â”‚ Ollama CLI         â”‚ ğŸ”„ ORCHESTRATES        â”‚
â”‚ batuta serve    â”‚ Remote APIs        â”‚ ğŸ”„ ORCHESTRATES        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ COMPUTE         â”‚                    â”‚                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ trueno          â”‚ PyTorch backend    â”‚ âš¡ ALTERNATIVE (SIMD)   â”‚
â”‚ trueno-gpu      â”‚ CUDA backend       â”‚ âš¡ ALTERNATIVE (native) â”‚
â”‚ repartir        â”‚ Ray/Anyscale       â”‚ âš¡ ALTERNATIVE (dist)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Legend:
  âš¡ ALTERNATIVE  - PAIML native replacement
  âœ“ COMPATIBLE   - Interoperates with format/API
  ğŸ”„ ORCHESTRATES - Batuta wraps/manages backend
```

```
[REVIEW-007] @sofia 2024-12-05
Toyota Principle: Nemawashi (Consensus Building)
Integration map clarifies when to use PAIML vs external backends.
Teams make informed decisions based on requirements.
Status: APPROVED
```

## Backend Selection Strategy

```rust
/// Select optimal backend based on requirements
pub struct BackendSelector {
    pub latency_requirement: LatencyTier,
    pub throughput_requirement: ThroughputTier,
    pub cost_sensitivity: CostTier,
    pub privacy_requirement: PrivacyTier,
}

#[derive(Debug, Clone, Copy)]
pub enum LatencyTier {
    RealTime,    // <100ms - local GPU or Groq
    Interactive, // <1s - local or fast remote
    Batch,       // >1s - any backend
}

#[derive(Debug, Clone, Copy)]
pub enum PrivacyTier {
    Sovereign,   // Local only, no external calls
    Private,     // VPC/dedicated endpoints
    Standard,    // Public APIs acceptable
}

impl BackendSelector {
    pub fn recommend(&self) -> Vec<ServingBackend> {
        match (self.latency_requirement, self.privacy_requirement) {
            (LatencyTier::RealTime, PrivacyTier::Sovereign) => {
                vec![ServingBackend::Realizar, ServingBackend::LlamaCpp]
            }
            (LatencyTier::RealTime, _) => {
                vec![ServingBackend::Groq, ServingBackend::Realizar]
            }
            (_, PrivacyTier::Sovereign) => {
                vec![ServingBackend::Realizar, ServingBackend::Ollama]
            }
            _ => {
                vec![ServingBackend::Together, ServingBackend::HuggingFace]
            }
        }
    }
}
```

```
[REVIEW-008] @miguel 2024-12-05
Toyota Principle: Pull System
Backend selection pulls optimal choice based on actual requirements.
No over-provisioning; right-sized infrastructure.
Status: APPROVED
```

## Quantization Support

| Backend | F32 | F16 | Q8 | Q4 | Q4_K_M | Q5_K_M | EXL2 |
|---------|-----|-----|----|----|--------|--------|------|
| realizar | âœ“ | âœ“ | âœ“ | âœ“ | âœ“ | âœ“ | âœ— |
| llama.cpp | âœ“ | âœ“ | âœ“ | âœ“ | âœ“ | âœ“ | âœ— |
| Ollama | âœ“ | âœ“ | âœ“ | âœ“ | âœ“ | âœ“ | âœ— |
| vLLM | âœ“ | âœ“ | âœ“ | âœ“ | âœ— | âœ— | âœ— |
| ExLlamaV2 | âœ— | âœ— | âœ— | âœ— | âœ— | âœ— | âœ“ |
| candle | âœ“ | âœ“ | âœ“ | âœ— | âœ— | âœ— | âœ— |

```
[REVIEW-009] @ana 2024-12-05
Toyota Principle: Muda Elimination (Waste)
Quantization reduces memory/compute waste.
Q4_K_M delivers 90% quality at 25% memory cost.
Status: APPROVED
```

## Implementation Plan

| Phase | Component | Description |
|-------|-----------|-------------|
| 1 | `src/serve/mod.rs` | Module structure |
| 2 | `src/serve/backends.rs` | Backend registry |
| 3 | `src/serve/local.rs` | Local backend integration |
| 4 | `src/serve/remote.rs` | Remote API clients |
| 5 | `src/serve/selector.rs` | Backend selection logic |
| 6 | `src/serve/tree.rs` | Tree visualization |
| 7 | CLI commands | `batuta serve *` |
| 8 | Tests | 95%+ coverage |

## Success Criteria

- [ ] `batuta serve start --backend realizar` launches server in <2s
- [ ] `batuta serve query` works across all backends
- [ ] `batuta serve tree` displays ecosystem in <100ms
- [ ] Backend auto-selection matches requirements
- [ ] Seamless failover between backends
- [ ] 95% test coverage on serve module

```
[REVIEW-010] @jorge 2024-12-05
Toyota Principle: Challenge (Long-term Vision)
Unified serving interface future-proofs against ecosystem churn.
New backends integrate without API changes.
Status: APPROVED
```

## Environment Variables

| Variable | Description | Default |
|----------|-------------|---------|
| `BATUTA_SERVE_BACKEND` | Default backend | `realizar` |
| `BATUTA_SERVE_PORT` | Default port | `8080` |
| `OLLAMA_HOST` | Ollama endpoint | `localhost:11434` |
| `OPENAI_API_KEY` | OpenAI key | None |
| `ANTHROPIC_API_KEY` | Anthropic key | None |
| `TOGETHER_API_KEY` | Together.ai key | None |
| `HF_TOKEN` | HuggingFace token | None |

## References

- llama.cpp: https://github.com/ggerganov/llama.cpp
- Ollama: https://ollama.ai
- vLLM: https://vllm.ai
- TGI: https://huggingface.co/docs/text-generation-inference
- candle: https://github.com/huggingface/candle
- GGUF spec: https://github.com/ggerganov/ggml/blob/master/docs/gguf.md
- Toyota Production System (Ohno, 1988)
- The Toyota Way (Liker, 2004)
